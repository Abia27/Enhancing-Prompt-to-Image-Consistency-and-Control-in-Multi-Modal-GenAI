# -*- coding: utf-8 -*-
"""GENAI_PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w-ah3X-ZcwhrIGl3dATS6-35D8MjNGO2

### ENHANCING PROMPT FOR IMAGE CONSISTENCY AND CONTROL IN MULTI-MODAL GENAI
"""

!pip install kaggle transformers sentencepiece torchvision pillow --quiet

import os
from google.colab import files

# Upload Kaggle API token
print("Upload Kaggle API Token (kaggle.json)")
files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

"""### Flickr8k Dataset"""

# STEP 1: Download Flickr8k Dataset from Kaggle
!kaggle datasets download -d adityajn105/flickr8k
!unzip -q flickr8k.zip -d flickr8k

# Image folder
img_dir = "/content/flickr8k/Images"

# STEP 2: Load Captions
import pandas as pd

captions_df = pd.read_csv("/content/flickr8k/captions.txt")
captions_df.head()

# STEP 3: Build Vocabulary + Tokenizer
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def encode_caption(caption, max_len=77):
    tokens = tokenizer(
        caption, padding="max_length", truncation=True, max_length=max_len, return_tensors="pt"
    )
    return tokens.input_ids.squeeze(), tokens.attention_mask.squeeze()

# STEP 4: Image Transform + Dataset Loader
import torch
from torchvision import transforms
from PIL import Image
from torch.utils.data import Dataset, DataLoader

tfm = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

class FlickrDataset(Dataset):
    def __init__(self, dataframe):
        self.df = dataframe

    def __len__(self): return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = os.path.join(img_dir, row['image'])
        image = tfm(Image.open(img_path).convert('RGB'))
        ids, mask = encode_caption(row['caption'])
        # Return image tensor, encoded caption IDs, attention mask, original caption, and image filename
        return image, ids, mask, row['caption'], row['image']

train_data = FlickrDataset(captions_df.sample(frac=0.8))
val_data = FlickrDataset(captions_df.drop(train_data.df.index))

train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
val_loader = DataLoader(val_data, batch_size=32)

!pip uninstall -y clip
!pip install ftfy regex tqdm
!pip install git+https://github.com/openai/CLIP.git

import torch.nn as nn
import torch.nn.functional as F
import clip

device = "cuda" if torch.cuda.is_available() else "cpu"
clip_model, preprocess = clip.load("ViT-B/32", device=device)
clip_model.eval()  # CLIP unfrozen
clip_model.float()  # Convert whole CLIP model to FP32
for p in clip_model.parameters():
    p.requires_grad = True  # Ensure frozen encoder remains stable


class CaptioningModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = clip_model.visual  # CLIP Vision Encoder (frozen or fine-tune lightly)
        self.attn = nn.MultiheadAttention(embed_dim=512, num_heads=4, batch_first=False)
        self.vis_proj = nn.Linear(768, 512)   # project CLIP patch tokens into decoder dimension

        for p in self.encoder.parameters():
            p.requires_grad = False

        self.embed = nn.Embedding(tokenizer.vocab_size, 512, padding_idx=tokenizer.pad_token_id)
        self.decoder = nn.GRU(512, 512, batch_first=True)
        self.fc = nn.Linear(512, tokenizer.vocab_size)

    def encode_patches(self, images):
        # CLIP ViT forward pass but stopping BEFORE pooling
        x = self.encoder.conv1(images)          # shape [B, C, H, W]
        x = x.reshape(x.shape[0], x.shape[1], -1).permute(0, 2, 1)  # [B, patches, C]
        # Fix: Ensure class_embedding is correctly shaped [B, 1, C] for concatenation
        class_embedding_expanded = self.encoder.class_embedding.unsqueeze(0).unsqueeze(0).expand(x.shape[0], -1, -1)
        x = torch.cat([class_embedding_expanded, x], dim=1)
        x = x + self.encoder.positional_embedding
        x = self.encoder.ln_pre(x)
        x = x.permute(1, 0, 2)                  # [seq, batch, embed]
        x = self.encoder.transformer(x)
        x = x.permute(1, 0, 2)                  # back to [batch, seq, embed]
        return x                                # return all patch tokens


    def forward(self, images, captions):

        # 1. Extract CLIP patch tokens for attention mechanism
        # And get the standard CLIP image embedding for alignment loss
        with torch.no_grad():
             vis_tokens_for_attn = self.encode_patches(images)  # [B, N, 768]
             img_emb_for_alignment = self.encoder(images) # Standard CLIP image embedding [B, 512]

        # 2. Project visual tokens to decoder dimension
        vis_tokens_for_attn = self.vis_proj(vis_tokens_for_attn)        # [B, N, 512]

        # 3. Embed caption tokens
        txt_emb = self.embed(captions)                # [B, T, 512]

        # 4. Prepare shapes for PyTorch attention
        # MultiHeadAttention expects [seq, batch, features]
        q = txt_emb.transpose(0, 1)                   # [T, B, 512]
        k = vis_tokens_for_attn.transpose(0, 1)                # [N, B, 512]
        v = vis_tokens_for_attn.transpose(0, 1)

        # 5. Apply attention: decoder queries → visual keys/values
        attn_out, attn_weights = self.attn(q, k, v)              # [T, B, 512]

        # 6. Back to batch-first for GRU
        attn_out = attn_out.transpose(0, 1)           # [B, T, 512]

        # 7. Decode sequence
        out, _ = self.decoder(attn_out)
        logits = self.fc(out)

        return logits, img_emb_for_alignment, attn_weights.transpose(0,1)


model = CaptioningModel().to(device)

criterion = nn.CrossEntropyLoss(
    ignore_index=tokenizer.pad_token_id,
    label_smoothing=0.1
)

optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)

import torch.nn.functional as F

# Stable Training Loop
def train_one_epoch():
    model.train()
    total_loss = 0

    for imgs, caption_ids, mask, refs, path in train_loader:
        imgs = imgs.to(device)
        caption_ids = caption_ids.to(device)

        logits, img_emb = model(imgs, caption_ids)

        # LM Loss
        lm_loss = criterion(
            logits[:, :-1, :].reshape(-1, tokenizer.vocab_size),
            caption_ids[:, 1:].reshape(-1)
        )

        # CLIP Alignment Loss
        with torch.no_grad():
            # Decode real strings for CLIP
            texts = [tokenizer.decode(c, skip_special_tokens=True) for c in caption_ids]
            txt_tokens = clip.tokenize(texts, truncate=True).to(device)
            txt_emb = clip_model.encode_text(txt_tokens).float()

        # Numerical stability: normalize manually
        img_norm = F.normalize(img_emb, dim=-1)
        txt_norm = F.normalize(txt_emb, dim=-1)

        align_loss = 1 - (img_norm * txt_norm).sum(dim=-1).mean()

        loss = lm_loss + 0.1 * align_loss

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        if torch.isnan(loss):
            print("Loss NaN detected — skipping this batch")
            continue

        total_loss += loss.item()

    return total_loss / len(train_loader)


for epoch in range(15):
    loss = train_one_epoch()
    print(f"Epoch {epoch+1}: Loss = {loss:.4f}")
    torch.save(model.state_dict(), "best_model.pt")

def train_one_epoch_tf(train_loader, model, optimizer, epoch, teacher_forcing_ratio=1.0):
    model.train()
    running_loss = 0.0

    for imgs, input_ids, attn_mask, refs, path in train_loader:
        imgs = imgs.to(device)
        input_ids = input_ids.to(device)

        batch_size, seq_len = input_ids.shape
        outputs = torch.zeros(batch_size, seq_len, tokenizer.vocab_size).to(device)

        # Use teacher forcing or predicted token
        decoder_input = input_ids[:, 0].unsqueeze(1)

        for t in range(seq_len - 1):
            logits, _ = model(imgs, decoder_input)
            logits = logits[:, -1:, :]  # last step

            outputs[:, t:t+1, :] = logits

            # sampling: decide next token
            use_tf = (torch.rand(1).item() < teacher_forcing_ratio)
            next_token = input_ids[:, t+1] if use_tf else logits.argmax(dim=-1).squeeze()

            decoder_input = torch.cat([decoder_input, next_token.unsqueeze(1)], dim=1)

        # Compute loss
        loss = criterion(outputs[:, :-1, :].reshape(-1, tokenizer.vocab_size),
                         input_ids[:, 1:].reshape(-1))

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch {epoch} | TF Ratio: {teacher_forcing_ratio:.2f} | Loss: {running_loss/len(train_loader):.4f}")
    return running_loss / len(train_loader)

# STEP 7: Generate Caption from an Image
import torch.nn.functional as F

def generate_caption(image_tensor, max_len=20):
    model.eval()
    image_tensor = image_tensor.unsqueeze(0).to(device).float()

    # Encode image once
    with torch.no_grad():
        img_emb = model.encoder(image_tensor)  # CLIP Image Embedding

    # Start with [CLS] as first token
    input_ids = torch.tensor([[tokenizer.cls_token_id]]).to(device)

    caption_tokens = []

    with torch.no_grad():
        for _ in range(max_len):
            # Embed current sequence
            txt_emb = model.embed(input_ids)
            out, _ = model.decoder(txt_emb)
            logits = model.fc(out[:, -1, :])  # last time step

            next_id = logits.argmax(dim=-1)
            next_token = next_id.item()

            if next_token == tokenizer.sep_token_id:
                break  # Stop when reaching end token

            caption_tokens.append(next_token)

            # Append new token to the sequence
            input_ids = torch.cat(
                [input_ids, next_id.unsqueeze(0)], dim=1
            )

    return tokenizer.decode(caption_tokens, skip_special_tokens=True)

print("ATTN SHAPE =", attn_weights.shape)

sample_img, _, _, _, _ = next(iter(val_loader))
caption = generate_caption(sample_img[0])
print("Generated caption:", caption)

# 9) Checkpoint helpers
def save_checkpoint(path, model, optimizer, epoch, extra=None):
    payload = {
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict() if optimizer is not None else None,
        "epoch": epoch,
        "extra": extra
    }
    torch.save(payload, path)
    print(f"Saved checkpoint: {path}")

def load_checkpoint(path, model, optimizer=None, map_location=device):
    ckpt = torch.load(path, map_location=map_location)
    model.load_state_dict(ckpt["model_state_dict"])
    if optimizer is not None and ckpt.get("optimizer_state_dict") is not None:
        optimizer.load_state_dict(ckpt["optimizer_state_dict"])
    epoch = ckpt.get("epoch", None)
    extra = ckpt.get("extra", None)
    print(f"Loaded checkpoint: {path}, epoch={epoch}")
    return epoch, extra

def save_best_model(model, optimizer, epoch, bleu_score, best_bleu, save_path="best_bleu.pt"):
    if bleu_score > best_bleu:
        torch.save({
            "model_state": model.state_dict(),
            "optimizer_state": optimizer.state_dict(),
            "epoch": epoch,
            "bleu": bleu_score
        }, save_path)
        print(f"New best BLEU: {bleu_score:.4f} — Model saved!")
        return bleu_score
    return best_bleu

# 10) Metrics: BLEU, METEOR, CLIPScore
import nltk
nltk.download('punkt')
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.translate.meteor_score import meteor_score

def compute_bleu(reference_list, candidate):
    ref_tokens = [nltk.word_tokenize(r.lower()) for r in reference_list]
    cand_tokens = nltk.word_tokenize(candidate.lower())
    smoothie = SmoothingFunction().method4
    try:
        return sentence_bleu(ref_tokens, cand_tokens, smoothing_function=smoothie)
    except Exception:
        return 0.0

# METEOR expects reference list & candidate string
def compute_meteor(reference_list, candidate):
    try:
        return meteor_score(reference_list, candidate)
    except Exception:
        return 0.0

# CLIPScore using CLIP model (ensure clip_model is float)
def compute_clip_score_pil(pil_image, caption_text):
    # pil_image: PIL.Image
    clip_model.eval()
    with torch.no_grad():
        img_in = preprocess(pil_image).unsqueeze(0).to(device) # Corrected from clip_preprocess to preprocess
        img_emb = clip_model.encode_image(img_in).float()
        txt_tokens = clip.tokenize([caption_text], truncate=True).to(device)
        txt_emb = clip_model.encode_text(txt_tokens).float()
        img_norm = img_emb / img_emb.norm(dim=-1, keepdim=True)
        txt_norm = txt_emb / txt_emb.norm(dim=-1, keepdim=True)
        score = (img_norm * txt_norm).sum(dim=-1).item()
    return float(score)

# small helper to convert tensor->PIL in evaluation
def tensor_to_pil(tensor):
    t = tensor.clone().cpu()
    # un-normalize (we used ImageNet normalization)
    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)
    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)
    t = t * std + mean
    t = (t * 255).clamp(0,255).byte().permute(1,2,0).numpy()
    return Image.fromarray(t)

import nltk
 nltk.download('punkt_tab')

# 11) Evaluation utility over val set (sample N images)
import ipywidgets as widgets
from IPython.display import display, clear_output
import matplotlib.pyplot as plt

def evaluate_model(model, val_loader, n_samples=200):
    model.eval()
    total_bleu = 0.0
    total_meteor = 0.0
    total_clip = 0.0
    count = 0
    examples = []
    with torch.no_grad():
        for batch in val_loader:
            imgs, ids, masks, refs, path = batch
            img = imgs[0]  # batch size 1 here
            refs_list = refs[0]  # list of reference captions
            pred = generate_caption(img)
            bleu = compute_bleu(refs_list, pred)
            meteor = compute_meteor(refs_list, pred)
            pil = tensor_to_pil(img)
            clip_s = compute_clip_score_pil(pil, pred)

            # Display image and details for the user
            print(f"\n--- Sample {count+1} ---")
            display(pil)
            print("Reference captions:", refs_list)
            print("Generated caption:", pred)
            print(f"Metrics -> BLEU: {bleu:.4f}, METEOR: {meteor:.4f}, CLIPScore: {clip_s:.4f}")

            total_bleu += bleu
            total_meteor += meteor
            total_clip += clip_s
            count += 1
            examples.append((pil, refs_list, pred, (bleu, meteor, clip_s)))
            if count >= n_samples:
                break
    return {
        "BLEU": total_bleu / max(1, count),
        "METEOR": total_meteor / max(1, count),
        "CLIPScore": total_clip / max(1, count),
        "n": count,
        "examples": examples
    }
evaluate_model(model, val_loader, n_samples=20)

from tqdm.auto import tqdm

# 12) Small training loop example with checkpointing
def train_one_epoch(train_loader, model, optimizer, epoch, grad_clip=1.0):
    model.train()
    running_loss = 0.0
    for imgs, input_ids, attention_mask, refs, path in tqdm(train_loader, desc=f"Epoch {epoch}"):
        imgs = imgs.to(device).float()
        input_ids = input_ids.to(device)
        # shift for teacher forcing: model returns logits for all time steps
        logits, _, _ = model(imgs, input_ids) # Unpack all three return values, only taking logits
        # compute loss: predict next token; shift inputs by 1
        lm_loss = criterion(logits[:, :-1, :].reshape(-1, tokenizer.vocab_size),
                            input_ids[:, 1:].reshape(-1))
        # optional small CLIP alignment can be added, but skip for stability here
        loss = lm_loss
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
        optimizer.step()
        running_loss += loss.item()
    avg_loss = running_loss / len(train_loader)
    print(f"Epoch {epoch} average loss: {avg_loss:.4f}")
    return avg_loss
train_one_epoch(train_loader, model, optimizer, epoch=0)

# 13) Stable Diffusion pipeline setup (requires HF token)
from diffusers import StableDiffusionPipeline
HF_TOKEN = os.environ.get("HF_TOKEN")
# Use runwayml/stable-diffusion-v1-5 (or any accessible SD model)
SD_MODEL = "runwayml/stable-diffusion-v1-5"
print("Loading Stable Diffusion model (this may take a while)...")
pipe = StableDiffusionPipeline.from_pretrained(SD_MODEL, torch_dtype=torch.float16, use_auth_token=HF_TOKEN)
pipe = pipe.to(device)
pipe.safety_checker = None
pipe.enable_attention_slicing()
try:
    pipe.enable_xformers_memory_efficient_attention()
except Exception:
    pass

# 14) SD regenerate helper
import torch.cuda.amp as amp

def regenerate_with_sd(prompt, num_inference_steps=20, guidance_scale=7.5, height=512, width=512):
    # Use autocast on CUDA to generate with float16
    if device == "cuda":
        with amp.autocast(device_type="cuda"):
            out = pipe(prompt, height=height, width=width,
                       num_inference_steps=num_inference_steps,
                       guidance_scale=guidance_scale)
            return out.images[0]
    else:
        out = pipe(prompt, height=height, width=width,
                   num_inference_steps=num_inference_steps,
                   guidance_scale=guidance_scale)
        return out.images[0]

# 15) Build a small cached validation list for UI (limit to first N for responsiveness)
VAL_UI_LIMIT = 500
val_examples = []
for i, (img_t, ids, mask, refs, path) in enumerate(val_data):
    val_examples.append((img_t, ids, refs, path))
    if i+1 >= VAL_UI_LIMIT:
        break
print("Prepared", len(val_examples), "validation examples for UI")

import nltk
nltk.download('punkt_tab')

import ipywidgets as widgets
from IPython.display import display, clear_output
import matplotlib.pyplot as plt

idx_slider = widgets.IntSlider(min=0, max=max(0, len(val_examples)-1), step=1, description="Index")
sd_steps = widgets.IntSlider(min=5, max=50, step=1, value=20, description="SD steps")
guidance = widgets.FloatSlider(min=1.0, max=15.0, step=0.5, value=7.5, description="Guidance")
sd_height = widgets.IntSlider(min=256, max=768, step=64, value=512, description="Height")
sd_width = widgets.IntSlider(min=256, max=768, step=64, value=512, description="Width")
generate_btn = widgets.Button(description="Generate SD from caption")
eval_btn = widgets.Button(description="Show example & metrics")
save_btn = widgets.Button(description="Save checkpoint")
load_btn = widgets.Button(description="Load checkpoint")
out = widgets.Output(layout={'border': '1px solid black'})

def show_example(idx):
    img_t, ids, refs, path = val_examples[idx]
    pil = tensor_to_pil(img_t)
    pred = generate_caption(img_t)
    # display
    plt.figure(figsize=(5,5))
    plt.imshow(pil)
    plt.axis("off")
    plt.title("Ground-truth (first):\n" + (refs if len(refs)>0 else "")) # refs is a string, not a list of strings
    plt.show()
    print("Model caption:", pred)
    bleu = compute_bleu(refs, pred)
    meteor = compute_meteor(refs, pred)
    clip_s = compute_clip_score_pil(pil, pred)
    print(f"Metrics -> BLEU: {bleu:.4f}, METEOR: {meteor:.4f}, CLIPScore: {clip_s:.4f}")
    return pred

def on_eval_click(b):
    with out:
        clear_output()
        idx = idx_slider.value
        show_example(idx)

def on_generate_click(b):
    with out:
        clear_output()
        idx = idx_slider.value
        pred = show_example(idx)
        print("\nGenerating Stable Diffusion image for the generated caption (may take 20-60s)...")
        sd_img = regenerate_with_sd(pred, num_inference_steps=sd_steps.value,
                                    guidance_scale=guidance.value, height=sd_height.value, width=sd_width.value)
        display(sd_img)

def on_save_click(b):
    with out:
        clear_output()
        save_checkpoint("flickr8k_caption_checkpoint.pt", model, optimizer, epoch=0, extra={"note":"ui-save"})
        print("Checkpoint saved.")

def on_load_click(b):
    with out:
        clear_output()
        path = "flickr8k_caption_checkpoint.pt"
        if os.path.exists(path):
            load_checkpoint(path, model, optimizer)
        else:
            print("No checkpoint at", path)

eval_btn.on_click(on_eval_click)
generate_btn.on_click(on_generate_click)
save_btn.on_click(on_save_click)
load_btn.on_click(on_load_click)

ui = widgets.VBox([
    widgets.HBox([idx_slider, sd_steps, guidance]),
    widgets.HBox([sd_height, sd_width]),
    widgets.HBox([generate_btn, eval_btn, save_btn, load_btn]),
    out
])
display(ui)

# show first example initially
with out:
    show_example(0)

import matplotlib.pyplot as plt
import numpy as np
from torchvision import transforms

def tensor_to_pil(tensor):
    tensor = tensor.cpu().detach().clamp(0,1)
    return transforms.ToPILImage()(tensor)

def visualize_attention(image_tensor, caption_text, attn_weights):
    print("Attention shape:", attn_weights.shape)   # Debug

    pil_img = tensor_to_pil(image_tensor)

    plt.figure(figsize=(12,6))

    # Left: image
    plt.subplot(1, 2, 1)
    plt.imshow(pil_img)
    plt.axis("off")

    # Handle batch dimension if present
    if attn_weights.dim() == 4:  # [B, heads, T, N]
        attn_weights = attn_weights[0]

    # Average across heads
    attn = attn_weights.mean(dim=0).cpu().numpy()

    # Right: heatmap
    plt.subplot(1, 2, 2)
    plt.imshow(attn, cmap='hot', interpolation='nearest', aspect='auto')
    plt.colorbar()
    plt.xlabel("Image Patches")
    plt.ylabel("Caption Tokens")
    plt.title("Attention Heatmap")

    tokens = caption_text.split()
    plt.yticks(np.arange(len(tokens)), tokens)

    plt.tight_layout()
    plt.show()

# Get a sample image and its ground truth caption from the validation set
sample_batch = next(iter(val_loader))

# Extract the first image tensor (unbatched) and its caption IDs from the batch
sample_img_tensor = sample_batch[0][0]
sample_caption_ids = sample_batch[1][0] # Ground truth token IDs

# Extract the first ground-truth caption text (the first reference from the list)
sample_caption_text_refs = sample_batch[3][0]
sample_caption_text = sample_caption_text_refs[0]

# Prepare image and caption for model's forward pass (add batch dimension)
model_input_img = sample_img_tensor.unsqueeze(0).to(device)
model_input_caption_ids = sample_caption_ids.unsqueeze(0).to(device)

# Set the model to evaluation mode and run a forward pass to get attention weights
model.eval()
with torch.no_grad():
    logits, img_emb_for_alignment, attn_weights = model(model_input_img, model_input_caption_ids)

# Visualize attention using the sample image, its ground-truth caption, and the obtained attention weights
visualize_attention(sample_img_tensor, sample_caption_text, attn_weights)



